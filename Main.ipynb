{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68a2c7a",
   "metadata": {},
   "source": [
    "# Employment Prediction Model\n",
    "\n",
    "This notebook builds a linear regression model to predict next week's employment levels based on spending patterns.\n",
    "\n",
    "## Overview\n",
    "- **Objective**: Predict employment (`emp_next_week`) from spending features\n",
    "- **Approach**: Linear regression with lagged spending features\n",
    "- **Data**: Affinity (daily spending) and Employment (weekly) datasets\n",
    "- **Train/Test Split**: 2021 and earlier for training, 2022+ for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e514175",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10220e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import all necessary libraries for data processing, modeling, and utilities.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296d1132",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "### Load Raw Data\n",
    "Load Affinity (daily spending) and Employment (weekly) datasets from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f9a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data from CSV files\n",
    "affinity_df = pd.read_csv('data/Affinity - State - Daily.csv')\n",
    "employment_df = pd.read_csv('data/Employment - State - Weekly.csv')\n",
    "\n",
    "print(f\"Affinity data shape: {affinity_df.shape}\")\n",
    "print(f\"Employment data shape: {employment_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e44e6",
   "metadata": {},
   "source": [
    "### Affinity Data Cleaning\n",
    "\n",
    "Convert daily spending data to weekly aggregates by:\n",
    "1. Creating proper datetime column from year/month/day\n",
    "2. Grouping by week ending on Friday\n",
    "3. Computing mean spending per state per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year, month, day columns to datetime\n",
    "affinity_df['date'] = pd.to_datetime(affinity_df[['year', 'month', 'day']])\n",
    "\n",
    "# Convert daily data to weekly by grouping by week ending on Friday\n",
    "# and calculating mean spending per state\n",
    "affinity_df['week_end'] = (\n",
    "    affinity_df['date']\n",
    "    .dt.to_period('W-FRI')\n",
    "    .dt.end_time\n",
    "    .dt.normalize()\n",
    ")\n",
    "aff_weekly = (\n",
    "    affinity_df\n",
    "    .groupby(['statefips', 'week_end'])\n",
    "    .mean(numeric_only=True)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(f\"Weekly affinity data shape: {aff_weekly.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da99752",
   "metadata": {},
   "source": [
    "### Employment Data Cleaning\n",
    "\n",
    "Convert employment data types and create datetime column to align with affinity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b64f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert year, month, day_endofweek to integer type\n",
    "for col in ['year', 'month', 'day_endofweek']:\n",
    "    employment_df[col] = employment_df[col].astype('int64')\n",
    "\n",
    "# Create datetime column from year, month, day components\n",
    "employment_df['date'] = pd.to_datetime({\n",
    "    'year': employment_df['year'],\n",
    "    'month': employment_df['month'],\n",
    "    'day': employment_df['day_endofweek']\n",
    "})\n",
    "\n",
    "print(f\"Employment data date range: {employment_df['date'].min()} to {employment_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7cfd12",
   "metadata": {},
   "source": [
    "### Merge Datasets\n",
    "\n",
    "Align the employment and affinity data on state and date for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470813ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge employment and affinity data on state and date\n",
    "merged = employment_df.merge(\n",
    "    aff_weekly, \n",
    "    left_on=['statefips', 'date'], \n",
    "    right_on=['statefips', 'week_end'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop the redundant week_end column\n",
    "merged = merged.drop(columns=['week_end'])\n",
    "\n",
    "print(f\"Merged data shape: {merged.shape}\")\n",
    "print(f\"Date range: {merged['date'].min()} to {merged['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36b878",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "### Process Spending Data\n",
    "\n",
    "Convert spending to numeric format and create lagged features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0e18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spend_all to numeric format (handles non-numeric values like '.')\n",
    "merged['spend_all'] = pd.to_numeric(merged['spend_all'].replace('.', pd.NA), errors='coerce')\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = merged['spend_all'].isna().sum()\n",
    "print(f\"Missing values in spend_all: {missing_count}\")\n",
    "\n",
    "# Create lagged features (previous week and 3 weeks ago spending)\n",
    "merged['spend_all_lag_1'] = merged.groupby('statefips')['spend_all'].shift(1)\n",
    "merged['spend_all_lag_3'] = merged.groupby('statefips')['spend_all'].shift(3)\n",
    "\n",
    "# Create target variable (employment for next week)\n",
    "merged['emp_next_week'] = merged.groupby('statefips')['emp'].apply(\n",
    "    lambda x: pd.to_numeric(x, errors='coerce')\n",
    ").shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43fb22",
   "metadata": {},
   "source": [
    "### Data Validation and Cleaning\n",
    "\n",
    "Remove rows with missing values in features or target to ensure model trains on complete data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d8d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for clarity\n",
    "feature_cols = ['spend_all', 'spend_all_lag_1', 'spend_all_lag_3']\n",
    "\n",
    "# Remove rows where ANY feature or target is NaN\n",
    "# This ensures we only train on complete data\n",
    "rows_before = len(merged)\n",
    "valid_rows = merged[feature_cols + ['emp_next_week']].notna().all(axis=1)\n",
    "merged = merged[valid_rows]\n",
    "\n",
    "rows_removed = rows_before - len(merged)\n",
    "print(f\"Rows before filtering: {rows_before}\")\n",
    "print(f\"Rows after filtering: {len(merged)}\")\n",
    "print(f\"Rows removed due to missing values: {rows_removed} ({rows_removed/rows_before*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c86cb78",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split\n",
    "\n",
    "Split data into training (â‰¤2021) and test (>2021) periods to simulate production performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0f95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split data based on temporal cutoff: 2021 for training, 2022+ for testing.\n",
    "This simulates real-world scenarios where we predict future data.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\nDate range in data: {merged['date'].min()} to {merged['date'].max()}\")\n",
    "\n",
    "# Split by year cutoff\n",
    "train = merged[merged['date'] <= '2021-12-31']\n",
    "test = merged[merged['date'] > '2021-12-31']\n",
    "\n",
    "print(f\"Train period: {train['date'].min()} to {train['date'].max()}\")\n",
    "print(f\"Train samples: {len(train)}\")\n",
    "print(f\"Test period: {test['date'].min() if len(test) > 0 else 'N/A'} to {test['date'].max() if len(test) > 0 else 'N/A'}\")\n",
    "print(f\"Test samples: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcffe0a2",
   "metadata": {},
   "source": [
    "## 5. Prepare Features for Modeling\n",
    "\n",
    "Extract feature and target matrices, with safety checks to ensure no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9789863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature and target matrices\n",
    "X_train = train[feature_cols]\n",
    "y_train = train['emp_next_week']\n",
    "\n",
    "X_test = test[feature_cols]\n",
    "y_test = test['emp_next_week']\n",
    "\n",
    "# Safety check: remove any remaining NaN rows (should be none after earlier filtering)\n",
    "train_mask = X_train.notna().all(axis=1) & y_train.notna()\n",
    "test_mask = X_test.notna().all(axis=1) & y_test.notna()\n",
    "\n",
    "X_train = X_train[train_mask]\n",
    "y_train = y_train[train_mask]\n",
    "X_test = X_test[test_mask]\n",
    "y_test = y_test[test_mask]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Target stats - Mean: {y_train.mean():.6f}, Std: {y_train.std():.6f}, Range: [{y_train.min():.6f}, {y_train.max():.6f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d022b7",
   "metadata": {},
   "source": [
    "## 6. Train Linear Regression Model\n",
    "\n",
    "Fit the model on training data and evaluate performance on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7f30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f'\\nModel Performance:')\n",
    "print(f'Mean Absolute Error (MAE): {mae:.6f}')\n",
    "if y_test.mean() != 0:\n",
    "    print(f'MAE as % of mean target: {mae / abs(y_test.mean()):.2%}')\n",
    "else:\n",
    "    print('MAE as % of mean target: N/A (mean is 0)')\n",
    "print(f'MAE as % of std target: {mae / y_test.std():.2%}')\n",
    "\n",
    "# Display model coefficients\n",
    "print(f'\\nModel Coefficients:')\n",
    "for col, coef in zip(feature_cols, model.coef_):\n",
    "    print(f'  {col}: {coef:.6f}')\n",
    "print(f'Intercept: {model.intercept_:.6f}')\n",
    "\n",
    "# Calculate baseline performance for comparison\n",
    "baseline_pred = np.full_like(y_test, y_train.mean())\n",
    "baseline_mae = mean_absolute_error(y_test, baseline_pred)\n",
    "improvement = ((baseline_mae - mae) / baseline_mae * 100) if baseline_mae > 0 else 0\n",
    "\n",
    "print(f'\\nBaseline Comparison:')\n",
    "print(f'Baseline (mean) MAE: {baseline_mae:.6f}')\n",
    "print(f'Model improvement over baseline: {improvement:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
